// !!! This is a file automatically generated by hipify!!!
/**
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

#pragma once

#include <hip/hip_runtime.h>

namespace dietgpu {

#if defined(__HIP_PLATFORM_AMD__)
// using WarpMaskT = unsigned long long;
using WarpMaskT = uint64_t;   // wave64
inline constexpr WarpMaskT kFullMask = 0xffffffffffffffffull;
#else
// using WarpMaskT = unsigned int;
using WarpMaskT = uint32_t;   // warp32
inline constexpr WarpMaskT kFullMask = 0xffffffffu;
#endif

// #if defined(__HIP_PLATFORM_AMD__)
// using WarpMaskT = uint64_t;   // wave64
// #else
// using WarpMaskT = uint32_t;   // warp32
// #endif


__device__ __forceinline__ WarpMaskT fullMask() {
#if defined(__HIP_PLATFORM_AMD__)
  return 0xffffffffffffffffull;
#else
  return 0xffffffffu;
#endif
}


__device__ __forceinline__ unsigned int
getBitfield(uint8_t val, int pos, int len) {
#if defined(__HIP_PLATFORM_AMD__)
  return ((uint32_t)val >> pos) & ((1u << len) - 1u);
#else
  unsigned int ret;
  asm("bfe.u32 %0, %1, %2, %3;"
      : "=r"(ret)
      : "r"((uint32_t)val), "r"(pos), "r"(len));
  return ret;
#endif
}

__device__ __forceinline__ unsigned int
getBitfield(uint16_t val, int pos, int len) {
#if defined(__HIP_PLATFORM_AMD__)
  return ((uint32_t)val >> pos) & ((1u << len) - 1u);
#else
  unsigned int ret;
  asm("bfe.u32 %0, %1, %2, %3;"
      : "=r"(ret)
      : "r"((uint32_t)val), "r"(pos), "r"(len));
  return ret;
#endif
}

__device__ __forceinline__ unsigned int
getBitfield(unsigned int val, int pos, int len) {
#if defined(__HIP_PLATFORM_AMD__)
  return (val >> pos) & ((1u << len) - 1u);
#else
  unsigned int ret;
  asm("bfe.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(val), "r"(pos), "r"(len));
  return ret;
#endif
}

__device__ __forceinline__ uint64_t
getBitfield(uint64_t val, int pos, int len) {
#if defined(__HIP_PLATFORM_AMD__)
  return (val >> pos) & ((1ull << len) - 1ull);
#else
  uint64_t ret;
  asm("bfe.u64 %0, %1, %2, %3;" : "=l"(ret) : "l"(val), "r"(pos), "r"(len));
  return ret;
#endif
}

__device__ __forceinline__ unsigned int
setBitfield(unsigned int val, unsigned int toInsert, int pos, int len) {
#if defined(__HIP_PLATFORM_AMD__)
  unsigned int mask = ((1u << len) - 1u) << pos;
  return (val & ~mask) | ((toInsert << pos) & mask);
#else
  unsigned int ret;
  asm("bfi.b32 %0, %1, %2, %3, %4;"
      : "=r"(ret)
      : "r"(toInsert), "r"(val), "r"(pos), "r"(len));
  return ret;
#endif
}

__device__ __forceinline__ uint32_t rotateLeft(uint32_t v, uint32_t shift) {
#if defined(__HIP_PLATFORM_AMD__)
  return (v << shift) | (v >> (32u - shift));
#else
  uint32_t out;
  asm("shf.l.clamp.b32 %0, %1, %2, %3;"
      : "=r"(out)
      : "r"(v), "r"(v), "r"(shift));
  return out;
#endif
}

__device__ __forceinline__ uint32_t rotateRight(uint32_t v, uint32_t shift) {
#if defined(__HIP_PLATFORM_AMD__)
  return (v >> shift) | (v << (32u - shift));
#else
  uint32_t out;
  asm("shf.r.clamp.b32 %0, %1, %2, %3;"
      : "=r"(out)
      : "r"(v), "r"(v), "r"(shift));
  return out;
#endif
}

__device__ __forceinline__ int getLaneId() {
#if defined(__HIP_PLATFORM_AMD__)
  return __lane_id();
#else
  int laneId;
  asm("mov.u32 %0, %%laneid;" : "=r"(laneId));
  return laneId;
#endif
}

__device__ __forceinline__ WarpMaskT getLaneMaskLt() {
#if defined(__HIP_PLATFORM_AMD__)
  int lane = __lane_id(); // 0..63
  return (lane == 0) ? 0ull : ((1ull << lane) - 1ull);
#else
  WarpMaskT mask;
  asm("mov.u32 %0, %%lanemask_lt;" : "=r"(mask));
  return mask;
#endif
}

__device__ __forceinline__ WarpMaskT getLaneMaskLe() {
#if defined(__HIP_PLATFORM_AMD__)
  int lane = __lane_id();
  return (lane == 63) ? 0xffffffffffffffffull : ((1ull << (lane + 1)) - 1ull);
#else
  WarpMaskT mask;
  asm("mov.u32 %0, %%lanemask_le;" : "=r"(mask));
  return mask;
#endif
}

__device__ __forceinline__ WarpMaskT getLaneMaskGt() {
#if defined(__HIP_PLATFORM_AMD__)
  int lane = __lane_id();
  return ~getLaneMaskLe();
#else
  WarpMaskT mask;
  asm("mov.u32 %0, %%lanemask_gt;" : "=r"(mask));
  return mask;
#endif
}

__device__ __forceinline__ WarpMaskT getLaneMaskGe() {
#if defined(__HIP_PLATFORM_AMD__)
  int lane = __lane_id();
  return ~getLaneMaskLt();
#else
  WarpMaskT mask;
  asm("mov.u32 %0, %%lanemask_ge;" : "=r"(mask));
  return mask;
#endif
}


// __device__ __forceinline__ unsigned getLaneMaskLt() {
// #if defined(__HIP_PLATFORM_AMD__)
//   int laneId = __lane_id();
//   return (1u << laneId) - 1u;
// #else
//   unsigned mask;
//   asm("mov.u32 %0, %%lanemask_lt;" : "=r"(mask));
//   return mask;
// #endif
// }

// __device__ __forceinline__ unsigned getLaneMaskLe() {
// #if defined(__HIP_PLATFORM_AMD__)
//   int laneId = __lane_id();
//   return (1u << (laneId + 1)) - 1u;
// #else
//   unsigned mask;
//   asm("mov.u32 %0, %%lanemask_le;" : "=r"(mask));
//   return mask;
// #endif
// }

// __device__ __forceinline__ unsigned getLaneMaskGt() {
// #if defined(__HIP_PLATFORM_AMD__)
//   int laneId = __lane_id();
//   return ~((1u << (laneId + 1)) - 1u);
// #else
//   unsigned mask;
//   asm("mov.u32 %0, %%lanemask_gt;" : "=r"(mask));
//   return mask;
// #endif
// }

// __device__ __forceinline__ unsigned getLaneMaskGe() {
// #if defined(__HIP_PLATFORM_AMD__)
//   int laneId = __lane_id();
//   return ~((1u << laneId) - 1u);
// #else
//   unsigned mask;
//   asm("mov.u32 %0, %%lanemask_ge;" : "=r"(mask));
//   return mask;
// #endif
// }

template <typename T>
__device__ inline T warpReduceAllMin(T val) {
#if __CUDA_ARCH__ >= 800
  return __reduce_min_sync(kFullMask, val);
#else
#pragma unroll
  for (int mask = kWarpSize / 2; mask > 0; mask >>= 1) {
    val = min(val, __shfl_xor_sync(kFullMask, val, mask, kWarpSize));
  }

  return val;
#endif
}

template <typename T, int Width = kWarpSize>
__device__ inline T warpReduceAllMax(T val) {
#if __CUDA_ARCH__ >= 800
  return __reduce_max_sync(kFullMask, val);
#else
#pragma unroll
  for (int mask = Width / 2; mask > 0; mask >>= 1) {
    val = max(val, __shfl_xor_sync(kFullMask, val, mask, kWarpSize));
  }

  return val;
#endif
}

template <typename T, int Width = kWarpSize>
__device__ inline T warpReduceAllSum(T val) {
#if __CUDA_ARCH__ >= 800
  return __reduce_add_sync(kFullMask, val);
#else
#pragma unroll
  for (int mask = Width / 2; mask > 0; mask >>= 1) {
    val += __shfl_xor_sync(kFullMask, val, mask, kWarpSize);
  }

  return val;
#endif
}

} // namespace dietgpu
